{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import re\n",
    "import time\n",
    "import datetime\n",
    "from string import punctuation\n",
    "import itertools as it\n",
    "import numpy as np\n",
    "from imageio import imwrite\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn.parameter as P\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torch.nn.utils.rnn as utils_rnn\n",
    "\n",
    "torch.multiprocessing.set_sharing_strategy('file_system')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23011601\n"
     ]
    }
   ],
   "source": [
    "! cat ../data/task3_train.txt | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_lines = np.random.choice(np.arange(23011601), size=150000, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_lines = np.sort(sample_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150000,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_lines.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 157,  181,  343,  609,  618,  655, 1115, 1132, 1151, 1296])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_lines[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_no = 0\n",
    "with open('/home/md359230/DeepLearning/assignment3/data/task3_sample.txt', 'w') as sample_data:\n",
    "    with open('/home/md359230/DeepLearning/assignment3/data/task3_train.txt', 'r') as full_data:\n",
    "        for counter, line in enumerate(full_data):\n",
    "            if line_no < 150000 and counter == sample_lines[line_no]:\n",
    "                line_no += 1\n",
    "                max_word_length = max([len(word) for word in line.split()])\n",
    "                correct_line = True if max_word_length <= 15 else False\n",
    "                if correct_line:\n",
    "                    sample_data.write(line)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150000\n"
     ]
    }
   ],
   "source": [
    "print(line_no)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131723\n"
     ]
    }
   ],
   "source": [
    "! cat ../data/task3_sample.txt | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/md359230/DeepLearning/assignment3/data/task3_sample.txt', 'r') as sample_data:\n",
    "    with open('/home/md359230/DeepLearning/assignment3/data/train.txt', 'w') as train_data:\n",
    "        with open('/home/md359230/DeepLearning/assignment3/data/validation.txt', 'w') as validation_data:\n",
    "            for counter, line in enumerate(sample_data):\n",
    "                if counter < 105378:\n",
    "                    train_data.write(line)            \n",
    "                else:\n",
    "                    validation_data.write(line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105378\n"
     ]
    }
   ],
   "source": [
    "! cat ../data/train.txt | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26345\n"
     ]
    }
   ],
   "source": [
    "! cat ../data/validation.txt | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CorpusPreprocessor(object):  \n",
    "    @staticmethod\n",
    "    def transform_text(text):\n",
    "        # Remove EOL character\n",
    "        text = text.replace('\\n', ' ')\n",
    "        \n",
    "        # Remove numbers\n",
    "        numbers = '1234567890'\n",
    "        for number in numbers:\n",
    "            text = text.replace(number, ' ')\n",
    "        \n",
    "        # Remove punctuation\n",
    "        for specialchar in punctuation:\n",
    "            text = text.replace(specialchar, ' ')\n",
    "            \n",
    "        # Remove double spaces\n",
    "        double_spaces = re.compile('\\s+')\n",
    "        text = re.sub(double_spaces, ' ', text)\n",
    "        \n",
    "        # Trim text\n",
    "        words = text.split(' ')\n",
    "        words = [word.lower() for word in words if word]\n",
    "        text = ' '.join(words)\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    @staticmethod\n",
    "    def mask_text(text, corpus_dictionary):\n",
    "        MASK = 'MASK'\n",
    "        masked_sent = text.split()\n",
    "        selected_word_idx = np.random.choice(len(masked_sent))\n",
    "        if np.random.choice(2) == 1:\n",
    "            original_word = masked_sent[selected_word_idx]\n",
    "            masked_sent[selected_word_idx] = MASK\n",
    "            return (masked_sent, original_word, 1)\n",
    "        else:\n",
    "            random_word = np.random.choice(corpus_dictionary)\n",
    "            masked_sent[selected_word_idx] = MASK\n",
    "            return (masked_sent, random_word, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'afdafs sa sfw gw g gfdsaf j'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CorpusPreprocessor.transform_text('AFDafs  41 2 5$#sa 24 Sfw15 gw g4gfdsaf j.    ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['MASK', 'ma', 'kota'], 'ala', 1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CorpusPreprocessor.mask_text(CorpusPreprocessor.transform_text('Ala ma kota.'), ['aaa', 'bbb', 'ccc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLPDataSet(Dataset):\n",
    "    def __init__(self, file_path):\n",
    "        self.sentences = []\n",
    "        self.corpus_vocab = []\n",
    "        \n",
    "        # Load corpus from file\n",
    "        chars = set()\n",
    "        with open(file_path, 'r') as raw_data:\n",
    "            for line in raw_data:\n",
    "                line = CorpusPreprocessor.transform_text(line)\n",
    "                chars = chars | set(line)\n",
    "                self.sentences.append(line)\n",
    "        \n",
    "        # Create dict with all letters\n",
    "        self.chars = OrderedDict(zip(chars, (torch.zeros(len(chars)) for _ in range(len(chars)))))\n",
    "        for idx, key in enumerate(self.chars.keys()):\n",
    "            if key != ' ':\n",
    "                self.chars[key][idx] = 1.\n",
    "        \n",
    "        # Fill the dictionary with sample 20%\n",
    "        for sentence in self.sentences[:len(self.sentences) // 5]:\n",
    "            for word in sentence.split():\n",
    "                if len(word) >= 3:\n",
    "                    self.corpus_vocab.append(word)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "    \n",
    "    def _encode_word(self, word):\n",
    "        result = []\n",
    "        if word == 'MASK':\n",
    "            return result\n",
    "        \n",
    "        for letter in word:\n",
    "            result.append(self.chars[letter])\n",
    "    \n",
    "        return result\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sentence, word, label = CorpusPreprocessor.mask_text(self.sentences[idx], self.corpus_vocab)\n",
    "        sentence = [self._encode_word(word) for word in sentence]\n",
    "        word = self._encode_word(word)\n",
    "        label = torch.LongTensor([label]).view(1)\n",
    "        return sentence, word, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = NLPDataSet('/home/md359230/DeepLearning/assignment3/data/train.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = NLPDataSet('/home/md359230/DeepLearning/assignment3/data/validation.txt')\n",
    "data_test.chars = data_train.chars.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jego żołnierza i biedę można znaleźć w repertuarze polskich teatrów'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.sentences[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 9, 1, 5, 5, 0, 1, 11, 8, 7]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(len, data_train[1][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nlp_collate(batch): \n",
    "    def stack_chars(word):\n",
    "        if len(word) > 1:\n",
    "            return torch.stack(word)\n",
    "        else:\n",
    "            if len(word) == 1:\n",
    "                return word[0].view(1, -1)\n",
    "            else:\n",
    "                return torch.zeros(1, 36)\n",
    "    \n",
    "    words = []\n",
    "    sentences = [] \n",
    "    words_length = []\n",
    "    sentences_length = []\n",
    "    labels = [] \n",
    "    for sentence, word, label in batch: \n",
    "        words.append(word)\n",
    "        sentences.append(sentence)\n",
    "        words_length.append(len(word))\n",
    "        sentences_length.append([len(word) if len(word) > 0 else 1 for word in sentence])\n",
    "        labels.append(label) \n",
    "       \n",
    "    words = [stack_chars(word) for word in words]\n",
    "    words = utils_rnn.pad_sequence(words)\n",
    "    sentences = [utils_rnn.pad_sequence(list(map(stack_chars, sentence))) for sentence in sentences]\n",
    "    words_length = torch.LongTensor(words_length)\n",
    "    sentences_length = [torch.LongTensor(sentence) for sentence in sentences_length]\n",
    "    labels = torch.stack(labels)\n",
    "    return sentences, sentences_length, words, words_length, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_train = DataLoader(data_train,\n",
    "                              batch_size=8,\n",
    "                              shuffle=True,\n",
    "                              collate_fn=nlp_collate,\n",
    "                              num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_test = DataLoader(data_test,\n",
    "                             batch_size=8,\n",
    "                             shuffle=True,\n",
    "                             collate_fn=nlp_collate,\n",
    "                             num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = next(iter(dataloader_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 13, 13, 8, 11, 11, 11, 11]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(s) for s in example[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([torch.Size([5, 6, 36]),\n",
       "  torch.Size([13, 25, 36]),\n",
       "  torch.Size([13, 38, 36]),\n",
       "  torch.Size([8, 20, 36]),\n",
       "  torch.Size([11, 10, 36]),\n",
       "  torch.Size([11, 12, 36]),\n",
       "  torch.Size([11, 6, 36]),\n",
       "  torch.Size([11, 15, 36])],\n",
       " [tensor(5),\n",
       "  tensor(13),\n",
       "  tensor(13),\n",
       "  tensor(8),\n",
       "  tensor(11),\n",
       "  tensor(11),\n",
       "  tensor(11),\n",
       "  tensor(11)])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[s.size() for s in example[0]], [max(e) for e in example[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for i, example in enumerate(data_train):\n",
    "    if i % 1000 == 0:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 18.8 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i, batch in enumerate(dataloader_train):\n",
    "    if i % 100 == 0:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_layer(in_features, \n",
    "                 out_feautres, \n",
    "                 batchnorm_module='default',\n",
    "                 activation_function='relu', \n",
    "                 *args, \n",
    "                 **kwargs):\n",
    "    activation_functions = nn.ModuleDict([\n",
    "        ['lrelu', nn.LeakyReLU()],\n",
    "        ['relu', nn.ReLU()],\n",
    "        ['sigmoid', nn.Sigmoid()]\n",
    "    ])\n",
    "\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(in_features, out_feautres, *args, **kwargs),\n",
    "        activation_functions[activation_function]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearStack(nn.Module):\n",
    "    \"\"\"Class containing implementation of standard linear stack.\n",
    "    \"\"\"\n",
    "    def __init__(self, sizes, n_classes, *args, **kwargs):\n",
    "        super(LinearStack, self).__init__()\n",
    "        self.linear_layers = nn.ModuleList([linear_layer(in_size, out_size, *args, **kwargs)\n",
    "                                                   for in_size, out_size in zip(sizes, sizes[1:])])\n",
    "        self.linear_layers.append(nn.Linear(sizes[-1], n_classes))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"Method implemention linear stack forward pass.\n",
    "        :param x: Input tensor.\n",
    "        :return: Processed tensor.\"\"\"\n",
    "        for linear_layer in self.linear_layers:\n",
    "            x = linear_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordEmbedder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(WordEmbedder, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(input_size=input_size,\n",
    "                            hidden_size=hidden_size)\n",
    "        \n",
    "    def forward(self, x): \n",
    "        words_input, lengths = x\n",
    "        words_input = utils_rnn.pack_padded_sequence(input=words_input, \n",
    "                                               lengths=lengths, \n",
    "                                               enforce_sorted=False)\n",
    "        out, _ = self.lstm(words_input)\n",
    "        result, _ = utils_rnn.pad_packed_sequence(out, \n",
    "                                            total_length=max(lengths).item())\n",
    "        idx = (torch.LongTensor(lengths) - 1).view(-1, 1)\\\n",
    "                                             .expand(len(lengths), \n",
    "                                                     self.hidden_size)\n",
    "        idx = idx.unsqueeze(0)\n",
    "        result = result.gather(0, idx).squeeze(0)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = WordEmbedder(36, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = next(iter(dataloader_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 8, 36])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example[2].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 3, 12, 11, 10,  5,  5,  8,  7])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 100])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = model((example[2], example[3]))\n",
    "out.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences, sentences_length = example[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([7, 7, 36]), tensor([7, 7, 6, 1, 5, 7, 7]), 7)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0].size(), sentences_length[0], max(sentences_length[0]).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = []\n",
    "for idx in range(len(sentences)):\n",
    "    embeddings.append(model((sentences[idx], sentences_length[idx])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([7, 100]),\n",
       " torch.Size([6, 100]),\n",
       " torch.Size([5, 100]),\n",
       " torch.Size([13, 100]),\n",
       " torch.Size([36, 100]),\n",
       " torch.Size([6, 100]),\n",
       " torch.Size([11, 100]),\n",
       " torch.Size([16, 100])]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[e.size() for e in embeddings]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MainLanguageModel(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_size,\n",
    "                 embedding_size,\n",
    "                 hidden_size, \n",
    "                 linear_sizes, \n",
    "                 n_classes):\n",
    "        super(MainLanguageModel, self).__init__()\n",
    "        self.embedding_size = embedding_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.wordembedder = WordEmbedder(input_size=input_size, \n",
    "                                         hidden_size=embedding_size)\n",
    "        self.lstm = nn.LSTM(input_size=embedding_size,\n",
    "                            hidden_size=hidden_size,\n",
    "                            bidirectional=True)\n",
    "        self.fc = LinearStack(linear_sizes, \n",
    "                              n_classes,     \n",
    "                              batchnorm_module='default',\n",
    "                              activation_function='lrelu')\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Create embeddings\n",
    "        sentences, sentences_length, words, words_length = x\n",
    "        embeddings = []\n",
    "        \n",
    "        for idx in range(len(sentences)):\n",
    "            embeddings.append(model((sentences[idx], sentences_length[idx])))\n",
    "                \n",
    "        masked_word_embeddings = self.wordembedder((words, words_length))\n",
    "        \n",
    "        \"\"\" # Create sentence embeddings\n",
    "        masked_sentence_embeddings = torch.stack(embeddings).squeeze(1)\n",
    "        masked_sentence_embeddings, _ = self.lstm(masked_sentence_embeddings)\n",
    "        masked_sentence_embeddings = masked_sentence_embeddings.view(-1, 1, 100, 2)\n",
    "\n",
    "        sequence_length = masked_sentence_embeddings.size(0)\n",
    "        if idx_masked == 0:\n",
    "            masked_sentence_forward = torch.zeros([1, self.hidden_size])\n",
    "        else:\n",
    "            masked_sentence_forward =  masked_sentence_embeddings[idx_masked - 1, :, :, 0]\n",
    "\n",
    "        if idx_masked == sequence_length - 1:\n",
    "            masked_sentence_backward = torch.zeros([1, self.hidden_size])\n",
    "        else:\n",
    "            masked_sentence_backward = masked_sentence_embeddings[idx_masked + 1, :, :, 1]\n",
    "\n",
    "        # Join sentence embeddings with true word embedding and apply FC layers\n",
    "        joined_tensor = torch.cat((masked_sentence_forward, masked_word_embedding.squeeze(0), masked_sentence_backward), \n",
    "                                  dim=1)\n",
    "        joined_tensor = self.fc(joined_tensor)\n",
    "        #joined_tensor = joined_tensor.squeeze()\n",
    "        return joined_tensor\"\"\"\n",
    "        return embeddings, masked_word_embeddings    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'input_size': 36,\n",
    "    'embedding_size': 100,\n",
    "    'hidden_size': 100,\n",
    "    'linear_sizes': (300, 100),\n",
    "    'n_classes': 2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlm = MainLanguageModel(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = next(iter(dataloader_train))\n",
    "sentences, sentences_length, words, words_length = example[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = mlm((sentences, sentences_length, words, words_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([torch.Size([6, 100]),\n",
       "  torch.Size([8, 100]),\n",
       "  torch.Size([22, 100]),\n",
       "  torch.Size([13, 100]),\n",
       "  torch.Size([12, 100]),\n",
       "  torch.Size([9, 100]),\n",
       "  torch.Size([7, 100]),\n",
       "  torch.Size([7, 100])],\n",
       " torch.Size([8, 100]))"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[o.size() for o in out[0]], out[1].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 816,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CrossEntropyLoss()"
      ]
     },
     "execution_count": 816,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.CrossEntropyLoss(out.view(1, -1), example[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 829,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetTrainer(object):\n",
    "    def __init__(self, trainLoader, testLoader):\n",
    "        self.trainLoader = trainLoader\n",
    "        self.testLoader = testLoader\n",
    "        \n",
    "    def assess(self,  net, use_gpu=False, device=None):\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        loader = self.trainLoader\n",
    "        for data in loader:\n",
    "            (sentence, word), label = data\n",
    "\n",
    "            if use_gpu:\n",
    "                if(device.__str__() != \"cpu\"):\n",
    "                    (sentence, word), label = ([[char.to(device) for char in word] for word in sentence], [char.to(device) for char in word]), label.to(device)\n",
    "                        \n",
    "            output = net((sentence, word))\n",
    "            _, predicted = torch.max(output.data, -1)\n",
    "            \n",
    "            total += 1\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        print('Accuracy of the network on {} train images: {:2.4f} %'.format(\n",
    "        len(self.trainLoader.dataset), 100 * correct / total))     \n",
    "           \n",
    "    def train(self, config, n_epoch=5, use_gpu=False):        \n",
    "        if use_gpu:\n",
    "            device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "            config['device'] = device\n",
    "            net = MainLanguageModel(**config)\n",
    "            \n",
    "            if device.__str__() != \"cpu\":\n",
    "                net.to(device)\n",
    "        else:\n",
    "            net = MainLanguageModel(**config)\n",
    "        \n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(net.parameters(), lr=0.0002, amsgrad=True)\n",
    "\n",
    "        for epoch in range(n_epoch):\n",
    "            running_loss = 0.0\n",
    "            t = time.time()\n",
    "            net.train()\n",
    "            \n",
    "            for i, data in enumerate(self.trainLoader, 0):\n",
    "                sentence = data[0]\n",
    "                word = data[1]\n",
    "                label = data[2]\n",
    "                \n",
    "                if use_gpu:\n",
    "                    if(device.__str__() != \"cpu\"):\n",
    "                        sentence = [[char.to(device) for char in word] for word in sentence]\n",
    "                        word = [char.to(device) for char in word]\n",
    "                        label = label.to(device)\n",
    "                        \n",
    "                output = net((sentence, word))\n",
    "                if i % 100 == 0:\n",
    "                    loss = criterion(output, label.view(1))\n",
    "                else:\n",
    "                    loss += criterion(output, label.view(1))\n",
    "                \n",
    "                running_loss += loss.item()\n",
    "                if i % 100 == 99:\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    \n",
    "                    now = datetime.datetime.now()\n",
    "                    print('[%s , %d, %5d] Loss: %.4f' %\n",
    "                          (now.strftime('%Y-%m-%d %H:%M:%S'), epoch + 1, i + 1, running_loss / 100))\n",
    "                    print('[%s , %d, %5d] Elapsed time: %2.4f s' %\n",
    "                          (now.strftime('%Y-%m-%d %H:%M:%S'), epoch + 1, i + 1, time.time() - t))\n",
    "                    running_loss = 0.0\n",
    "                    t = time.time()\n",
    "            net.eval()\n",
    "            \n",
    "            self.assess(net, use_gpu=use_gpu, device=device)\n",
    "            self.validate(net, use_gpu=True, device=device)\n",
    "        return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 830,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'input_size': 36,\n",
    "    'embedding_size': 100,\n",
    "    'hidden_size': 100,\n",
    "    'linear_sizes': (300, 100),\n",
    "    'n_classes': 2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 831,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = NetTrainer(dataloader_train, dataloader_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 832,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-06-02 14:18:47 , 1,   100] Loss: 34.7083\n",
      "[2019-06-02 14:18:47 , 1,   100] Elapsed time: 34.3701 s\n",
      "[2019-06-02 14:20:40 , 1,   200] Loss: 34.8194\n",
      "[2019-06-02 14:20:40 , 1,   200] Elapsed time: 113.1188 s\n",
      "[2019-06-02 14:22:17 , 1,   300] Loss: 35.0395\n",
      "[2019-06-02 14:22:17 , 1,   300] Elapsed time: 97.4119 s\n",
      "[2019-06-02 14:23:50 , 1,   400] Loss: 35.2104\n",
      "[2019-06-02 14:23:50 , 1,   400] Elapsed time: 92.9559 s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-832-7fbf91dca436>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-829-3b4577e14f02>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, config, n_epoch, use_gpu)\u001b[0m\n\u001b[1;32m     54\u001b[0m                         \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m                     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/deep_learning/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-798-5c370d4930ad>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     31\u001b[0m                 \u001b[0membeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m                 \u001b[0membeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwordembedder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mmasked_word_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwordembedder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmasked_word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/deep_learning/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-734-2d05be9e857f>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     16\u001b[0m                 \u001b[0mhidden\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mletter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/deep_learning/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/deep_learning/lib/python3.5/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m             result = _impl(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0;32m--> 179\u001b[0;31m                            self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             result = _impl(input, batch_sizes, hx, self._flat_weights, self.bias,\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train(config, n_epoch=1, use_gpu=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network visualzation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing zone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = set()\n",
    "corpus_words = set()\n",
    "lens = list()\n",
    "with open('/home/md359230/DeepLearning/assignment3/data/task3_sample.txt', 'r') as sample_data:\n",
    "    for line in sample_data:\n",
    "        line = CorpusPreprocessor.transform_text(line)\n",
    "        lens.append(len(line))\n",
    "        chars = chars | set(line)\n",
    "        words = line.split()\n",
    "        corpus_words = corpus_words | set(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([7.0255e+04, 2.5162e+04, 3.7240e+03, 6.8500e+02, 1.3800e+02,\n",
       "        2.6000e+01, 5.0000e+00, 4.0000e+00, 0.0000e+00, 1.0000e+00]),\n",
       " array([   4.,  137.,  270.,  403.,  536.,  669.,  802.,  935., 1068.,\n",
       "        1201., 1334.]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'po wybuchu ii wojny światowej podczas kampanii wrześniowej sprawował stanowisko dowódcy pułku piechoty'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = 'Po wybuchu II wojny światowej 1939 podczas kampanii wrześniowej sprawował stanowisko dowódcy 78 pułku piechoty.'\n",
    "cp.transform_text(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        self.dictionary = OrderedDict()\n",
    "        self.corpus_path = corpus_path\n",
    "        with open(corpus_path, 'r') as corpus:\n",
    "            for line in corpus:\n",
    "                self.dictionary[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'afs 41 2 5$#sa 24 Sfw15 gw g4gfdsaf j'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "double_spaces = re.compile('\\s+')\n",
    "re.sub(double_spaces, ' ', 'afs  41 2 5$#sa 24 Sfw15 gw g4gfdsaf j')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
