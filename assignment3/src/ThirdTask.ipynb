{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import re\n",
    "import time\n",
    "import datetime\n",
    "from string import punctuation\n",
    "import itertools as it\n",
    "import numpy as np\n",
    "from imageio import imwrite\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import OrderedDict\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn.parameter as P\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torch.nn.utils.rnn as utils_rnn\n",
    "\n",
    "torch.multiprocessing.set_sharing_strategy('file_system')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23011601\n"
     ]
    }
   ],
   "source": [
    "! cat ../data/task3_train.txt | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_lines = np.random.choice(np.arange(23011601), size=150000, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_lines = np.sort(sample_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150000,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_lines.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 157,  181,  343,  609,  618,  655, 1115, 1132, 1151, 1296])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_lines[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_no = 0\n",
    "with open('/home/md359230/DeepLearning/assignment3/data/task3_sample.txt', 'w') as sample_data:\n",
    "    with open('/home/md359230/DeepLearning/assignment3/data/task3_train.txt', 'r') as full_data:\n",
    "        for counter, line in enumerate(full_data):\n",
    "            if line_no < 150000 and counter == sample_lines[line_no]:\n",
    "                line_no += 1\n",
    "                max_word_length = max([len(word) for word in line.split()])\n",
    "                correct_line = True if max_word_length <= 15 else False\n",
    "                if correct_line:\n",
    "                    sample_data.write(line)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150000\n"
     ]
    }
   ],
   "source": [
    "print(line_no)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131723\n"
     ]
    }
   ],
   "source": [
    "! cat ../data/task3_sample.txt | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/md359230/DeepLearning/assignment3/data/task3_sample.txt', 'r') as sample_data:\n",
    "    with open('/home/md359230/DeepLearning/assignment3/data/train.txt', 'w') as train_data:\n",
    "        with open('/home/md359230/DeepLearning/assignment3/data/validation.txt', 'w') as validation_data:\n",
    "            for counter, line in enumerate(sample_data):\n",
    "                if counter < 105378:\n",
    "                    train_data.write(line)            \n",
    "                else:\n",
    "                    validation_data.write(line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105378\n"
     ]
    }
   ],
   "source": [
    "! cat ../data/train.txt | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26345\n"
     ]
    }
   ],
   "source": [
    "! cat ../data/validation.txt | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CorpusPreprocessor(object):  \n",
    "    @staticmethod\n",
    "    def transform_text(text):\n",
    "        # Remove EOL character\n",
    "        text = text.replace('\\n', ' ')\n",
    "        \n",
    "        # Remove numbers\n",
    "        numbers = '1234567890'\n",
    "        for number in numbers:\n",
    "            text = text.replace(number, ' ')\n",
    "        \n",
    "        # Remove punctuation\n",
    "        for specialchar in punctuation:\n",
    "            text = text.replace(specialchar, ' ')\n",
    "            \n",
    "        # Remove double spaces\n",
    "        double_spaces = re.compile('\\s+')\n",
    "        text = re.sub(double_spaces, ' ', text)\n",
    "        \n",
    "        # Trim text\n",
    "        words = text.split(' ')\n",
    "        words = [word.lower() for word in words if word]\n",
    "        text = ' '.join(words)\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    @staticmethod\n",
    "    def mask_text(text, corpus_dictionary):\n",
    "        MASK = 'MASK'\n",
    "        masked_sent = text.split()\n",
    "        selected_word_idx = random.randint(0, len(masked_sent) - 1)\n",
    "        if random.randint(0, 1) == 1:\n",
    "            original_word = masked_sent[selected_word_idx]\n",
    "            masked_sent[selected_word_idx] = MASK\n",
    "            return (masked_sent, original_word, selected_word_idx, 1)\n",
    "        else:\n",
    "            random_word = np.random.choice(corpus_dictionary)\n",
    "            masked_sent[selected_word_idx] = MASK\n",
    "            return (masked_sent, random_word, selected_word_idx, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'afdafs sa sfw gw g gfdsaf j'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CorpusPreprocessor.transform_text('AFDafs  41 2 5$#sa 24 Sfw15 gw g4gfdsaf j.    ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['ala', 'MASK', 'kota'], 'aaa', 1, 0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CorpusPreprocessor.mask_text(CorpusPreprocessor.transform_text('Ala ma kota.'), ['aaa', 'bbb', 'ccc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLPDataSet(Dataset):\n",
    "    def __init__(self, file_path):\n",
    "        self.sentences = []\n",
    "        self.corpus_vocab = []\n",
    "        \n",
    "        # Load corpus from file\n",
    "        chars = set()\n",
    "        with open(file_path, 'r') as raw_data:\n",
    "            for line in raw_data:\n",
    "                line = CorpusPreprocessor.transform_text(line)\n",
    "                chars = chars | set(line)\n",
    "                self.sentences.append(line)\n",
    "        \n",
    "        # Create dict with all letters\n",
    "        self.chars = OrderedDict(zip(chars, (torch.zeros(len(chars)) for _ in range(len(chars)))))\n",
    "        for idx, key in enumerate(self.chars.keys()):\n",
    "            if key != ' ':\n",
    "                self.chars[key][idx] = 1.\n",
    "        \n",
    "        # Fill the dictionary with sample 20%\n",
    "        for sentence in self.sentences[:len(self.sentences) // 5]:\n",
    "            for word in sentence.split():\n",
    "                if len(word) >= 3:\n",
    "                    self.corpus_vocab.append(word)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "    \n",
    "    def _encode_word(self, word):\n",
    "        result = []\n",
    "        if word == 'MASK':\n",
    "            return result\n",
    "        \n",
    "        for letter in word:\n",
    "            result.append(self.chars[letter])\n",
    "    \n",
    "        return result\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sentence, word, idx, label = CorpusPreprocessor.mask_text(self.sentences[idx], self.corpus_vocab)\n",
    "        sentence = [self._encode_word(word) for word in sentence]\n",
    "        word = self._encode_word(word)\n",
    "        idx = torch.LongTensor([idx]).view(1)\n",
    "        label = torch.LongTensor([label]).view(1)\n",
    "        return sentence, word, idx, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = NLPDataSet('/home/md359230/DeepLearning/assignment3/data/train.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = NLPDataSet('/home/md359230/DeepLearning/assignment3/data/validation.txt')\n",
    "data_test.chars = data_train.chars.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jego żołnierza i biedę można znaleźć w repertuarze polskich teatrów'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.sentences[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 9, 0, 5, 5, 7, 1, 11, 8, 7]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(len, data_train[1][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nlp_collate(batch): \n",
    "    def stack_chars(word):\n",
    "        if len(word) > 1:\n",
    "            return torch.stack(word)\n",
    "        else:\n",
    "            if len(word) == 1:\n",
    "                return word[0].view(1, -1)\n",
    "            else:\n",
    "                return torch.zeros(1, 36)\n",
    "    \n",
    "    words = []\n",
    "    sentences = [] \n",
    "    words_length = []\n",
    "    sentences_length = []\n",
    "    idxs = []\n",
    "    labels = [] \n",
    "    for sentence, word, idx, label in batch: \n",
    "        words.append(word)\n",
    "        sentences.append(sentence)\n",
    "        words_length.append(len(word))\n",
    "        sentences_length.append([len(word) if len(word) > 0 else 1 for word in sentence])\n",
    "        idxs.append(idx)\n",
    "        labels.append(label) \n",
    "       \n",
    "    words = [stack_chars(word) for word in words]\n",
    "    words = utils_rnn.pad_sequence(words)\n",
    "    sentences = [utils_rnn.pad_sequence(list(map(stack_chars, sentence))) for sentence in sentences]\n",
    "    words_length = torch.LongTensor(words_length)\n",
    "    sentences_length = [torch.LongTensor(sentence) for sentence in sentences_length]\n",
    "    idxs = torch.stack(idxs)\n",
    "    labels = torch.stack(labels).squeeze(1)\n",
    "    return sentences, sentences_length, words, words_length, idxs,  labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_train = DataLoader(data_train,\n",
    "                              batch_size=1024,\n",
    "                              shuffle=True,\n",
    "                              collate_fn=nlp_collate,\n",
    "                              num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_test = DataLoader(data_test,\n",
    "                             batch_size=1024,\n",
    "                             shuffle=True,\n",
    "                             collate_fn=nlp_collate,\n",
    "                             num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = next(iter(dataloader_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[12,\n",
       " 15,\n",
       " 13,\n",
       " 10,\n",
       " 11,\n",
       " 10,\n",
       " 10,\n",
       " 13,\n",
       " 13,\n",
       " 9,\n",
       " 13,\n",
       " 10,\n",
       " 8,\n",
       " 15,\n",
       " 13,\n",
       " 9,\n",
       " 8,\n",
       " 14,\n",
       " 12,\n",
       " 10,\n",
       " 15,\n",
       " 15,\n",
       " 8,\n",
       " 10,\n",
       " 13,\n",
       " 11,\n",
       " 10,\n",
       " 10,\n",
       " 9,\n",
       " 13,\n",
       " 11,\n",
       " 11,\n",
       " 12,\n",
       " 14,\n",
       " 14,\n",
       " 11,\n",
       " 12,\n",
       " 12,\n",
       " 15,\n",
       " 11,\n",
       " 10,\n",
       " 12,\n",
       " 12,\n",
       " 11,\n",
       " 9,\n",
       " 10,\n",
       " 8,\n",
       " 12,\n",
       " 9,\n",
       " 14,\n",
       " 12,\n",
       " 9,\n",
       " 12,\n",
       " 14,\n",
       " 8,\n",
       " 11,\n",
       " 10,\n",
       " 10,\n",
       " 6,\n",
       " 13,\n",
       " 11,\n",
       " 12,\n",
       " 15,\n",
       " 9,\n",
       " 9,\n",
       " 12,\n",
       " 10,\n",
       " 9,\n",
       " 14,\n",
       " 12,\n",
       " 10,\n",
       " 13,\n",
       " 13,\n",
       " 15,\n",
       " 14,\n",
       " 10,\n",
       " 11,\n",
       " 9,\n",
       " 9,\n",
       " 13,\n",
       " 14,\n",
       " 11,\n",
       " 13,\n",
       " 12,\n",
       " 10,\n",
       " 12,\n",
       " 14,\n",
       " 13,\n",
       " 8,\n",
       " 13,\n",
       " 13,\n",
       " 14,\n",
       " 13,\n",
       " 10,\n",
       " 12,\n",
       " 15,\n",
       " 10,\n",
       " 10,\n",
       " 12,\n",
       " 5,\n",
       " 15,\n",
       " 11,\n",
       " 8,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 13,\n",
       " 10,\n",
       " 12,\n",
       " 14,\n",
       " 13,\n",
       " 11,\n",
       " 10,\n",
       " 9,\n",
       " 12,\n",
       " 11,\n",
       " 9,\n",
       " 7,\n",
       " 11,\n",
       " 13,\n",
       " 12,\n",
       " 11,\n",
       " 11,\n",
       " 14,\n",
       " 10,\n",
       " 10,\n",
       " 14,\n",
       " 12]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(s) for s in example[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([torch.Size([13, 10, 36]),\n",
       "  torch.Size([13, 23, 36]),\n",
       "  torch.Size([9, 11, 36]),\n",
       "  torch.Size([9, 12, 36]),\n",
       "  torch.Size([10, 4, 36]),\n",
       "  torch.Size([14, 15, 36]),\n",
       "  torch.Size([12, 15, 36]),\n",
       "  torch.Size([12, 9, 36]),\n",
       "  torch.Size([12, 19, 36]),\n",
       "  torch.Size([12, 12, 36]),\n",
       "  torch.Size([12, 21, 36]),\n",
       "  torch.Size([12, 25, 36]),\n",
       "  torch.Size([9, 14, 36]),\n",
       "  torch.Size([15, 17, 36]),\n",
       "  torch.Size([14, 24, 36]),\n",
       "  torch.Size([9, 9, 36])],\n",
       " [tensor(13),\n",
       "  tensor(13),\n",
       "  tensor(9),\n",
       "  tensor(9),\n",
       "  tensor(10),\n",
       "  tensor(14),\n",
       "  tensor(12),\n",
       "  tensor(12),\n",
       "  tensor(12),\n",
       "  tensor(12),\n",
       "  tensor(12),\n",
       "  tensor(12),\n",
       "  tensor(9),\n",
       "  tensor(15),\n",
       "  tensor(14),\n",
       "  tensor(9)])"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[s.size() for s in example[0]], [max(e) for e in example[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([torch.Size([10]),\n",
       "  torch.Size([23]),\n",
       "  torch.Size([11]),\n",
       "  torch.Size([12]),\n",
       "  torch.Size([4]),\n",
       "  torch.Size([15]),\n",
       "  torch.Size([15]),\n",
       "  torch.Size([9]),\n",
       "  torch.Size([19]),\n",
       "  torch.Size([12]),\n",
       "  torch.Size([21]),\n",
       "  torch.Size([25]),\n",
       "  torch.Size([14]),\n",
       "  torch.Size([17]),\n",
       "  torch.Size([24]),\n",
       "  torch.Size([9])],\n",
       " tensor([[ 1],\n",
       "         [ 7],\n",
       "         [ 5],\n",
       "         [ 0],\n",
       "         [ 2],\n",
       "         [13],\n",
       "         [ 0],\n",
       "         [ 2],\n",
       "         [ 8],\n",
       "         [11],\n",
       "         [17],\n",
       "         [15],\n",
       "         [10],\n",
       "         [ 9],\n",
       "         [20],\n",
       "         [ 2]]))"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[e.size() for e in example[1]], example[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for i, example in enumerate(data_train):\n",
    "    if i % 1000 == 0:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "3900\n",
      "4000\n",
      "4100\n",
      "4200\n",
      "4300\n",
      "4400\n",
      "4500\n",
      "4600\n",
      "4700\n",
      "4800\n",
      "4900\n",
      "5000\n",
      "5100\n",
      "5200\n",
      "5300\n",
      "5400\n",
      "5500\n",
      "5600\n",
      "5700\n",
      "5800\n",
      "5900\n",
      "6000\n",
      "6100\n",
      "6200\n",
      "6300\n",
      "6400\n",
      "6500\n",
      "CPU times: user 16.7 s, sys: 11.6 s, total: 28.4 s\n",
      "Wall time: 8min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i, batch in enumerate(dataloader_train):\n",
    "    if i % 1000 == 0:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_layer(in_features, \n",
    "                 out_features, \n",
    "                 batchnorm_module='default',\n",
    "                 activation_function='relu', \n",
    "                 *args, \n",
    "                 **kwargs):\n",
    "    activation_functions = nn.ModuleDict([\n",
    "        ['lrelu', nn.LeakyReLU()],\n",
    "        ['relu', nn.ReLU()],\n",
    "        ['sigmoid', nn.Sigmoid()]\n",
    "    ])\n",
    "\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(in_features, out_features, *args, **kwargs),\n",
    "        nn.BatchNorm1d(out_features),\n",
    "        activation_functions[activation_function]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearStack(nn.Module):\n",
    "    \"\"\"Class containing implementation of standard linear stack.\n",
    "    \"\"\"\n",
    "    def __init__(self, sizes, n_classes, *args, **kwargs):\n",
    "        super(LinearStack, self).__init__()\n",
    "        self.linear_layers = nn.ModuleList([linear_layer(in_size, out_size, *args, **kwargs)\n",
    "                                                   for in_size, out_size in zip(sizes, sizes[1:])])\n",
    "        self.linear_layers.append(nn.Linear(sizes[-1], n_classes))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"Method implemention linear stack forward pass.\n",
    "        :param x: Input tensor.\n",
    "        :return: Processed tensor.\"\"\"\n",
    "        for linear_layer in self.linear_layers:\n",
    "            x = linear_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordEmbedder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(WordEmbedder, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(input_size=input_size,\n",
    "                            hidden_size=hidden_size)\n",
    "        \n",
    "    def forward(self, x): \n",
    "        words_input, lengths = x\n",
    "        words_input = utils_rnn.pack_padded_sequence(input=words_input, \n",
    "                                               lengths=lengths, \n",
    "                                               enforce_sorted=False)\n",
    "        out, _ = self.lstm(words_input)\n",
    "        result, _ = utils_rnn.pad_packed_sequence(out, \n",
    "                                            total_length=max(lengths).item())\n",
    "        idx = (lengths - 1).view(-1, 1)\\\n",
    "                           .expand(len(lengths), \n",
    "                                   self.hidden_size)\n",
    "        idx = idx.unsqueeze(0)\n",
    "        result = result.gather(0, idx).squeeze(0)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = WordEmbedder(36, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = next(iter(dataloader_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "example2 = example[2].to(device) \n",
    "example3 = example[3].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model((example2, example3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model((example[2], example[3]))\n",
    "out.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences, sentences_length = example[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([15, 100]),\n",
       " torch.Size([13, 100]),\n",
       " torch.Size([23, 100]),\n",
       " torch.Size([22, 100]),\n",
       " torch.Size([42, 100]),\n",
       " torch.Size([24, 100]),\n",
       " torch.Size([12, 100]),\n",
       " torch.Size([20, 100]),\n",
       " torch.Size([13, 100]),\n",
       " torch.Size([8, 100]),\n",
       " torch.Size([23, 100]),\n",
       " torch.Size([16, 100]),\n",
       " torch.Size([8, 100]),\n",
       " torch.Size([7, 100]),\n",
       " torch.Size([14, 100]),\n",
       " torch.Size([11, 100])]"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = []\n",
    "for idx in range(len(sentences)):\n",
    "    embeddings.append(model((sentences[idx], sentences_length[idx])))\n",
    "\n",
    "[e.size() for e in embeddings]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MainLanguageModel(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_size,\n",
    "                 embedding_size,\n",
    "                 hidden_size, \n",
    "                 linear_sizes, \n",
    "                 n_classes):\n",
    "        super(MainLanguageModel, self).__init__()\n",
    "        self.embedding_size = embedding_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.wordembedder = WordEmbedder(input_size=input_size, \n",
    "                                         hidden_size=embedding_size)\n",
    "        self.lstm = nn.LSTM(input_size=embedding_size,\n",
    "                            hidden_size=hidden_size,\n",
    "                            bidirectional=True)\n",
    "        self.fc = LinearStack(linear_sizes, \n",
    "                              n_classes,     \n",
    "                              batchnorm_module='default',\n",
    "                              activation_function='lrelu')\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Create embeddings\n",
    "        sentences, sentences_length, words, words_length, idxs = x\n",
    "        embeddings = []\n",
    "        \n",
    "        for idx in range(len(sentences)):\n",
    "            embeddings.append(self.wordembedder((sentences[idx], sentences_length[idx])))\n",
    "                \n",
    "        masked_word_embeddings = self.wordembedder((words, words_length))\n",
    "        \n",
    "        # Feed RNN with sentence batch\n",
    "        lengths = torch.LongTensor([sl.size()[0] for sl in sentences_length])\n",
    "        sentence_batch = utils_rnn.pad_sequence(embeddings)        \n",
    "        sentence_batch = utils_rnn.pack_padded_sequence(input=sentence_batch, \n",
    "                                                        lengths=lengths, \n",
    "                                                        enforce_sorted=False)\n",
    "        \n",
    "        out, _ = self.lstm(sentence_batch)\n",
    "        result, _ = utils_rnn.pad_packed_sequence(out, \n",
    "                                                  total_length=max(lengths).item())\n",
    "        idx = (idxs).view(-1, 1)\\\n",
    "                    .expand(len(lengths), 2 * self.hidden_size)        \n",
    "        idx = idx.unsqueeze(0)\n",
    "        result = result.gather(0, idx).squeeze(0)\n",
    "        \n",
    "        joined_result = torch.cat((result, masked_word_embeddings), dim=1)\n",
    "        joined_result = self.fc(joined_result)\n",
    "            \n",
    "        # Return embeddings, masked_word_embeddings, sentences_length, idxs\n",
    "        return joined_result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'input_size': 36,\n",
    "    'embedding_size': 100,\n",
    "    'hidden_size': 50,\n",
    "    'linear_sizes': (200, 100),\n",
    "    'n_classes': 2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlm = MainLanguageModel(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "mlm = mlm.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = next(iter(dataloader_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences, sentences_length, words, words_length, idxs, labels = example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [sentence.to(device) for sentence in sentences]\n",
    "sentences_length = [sl.to(device) for sl in sentences_length]\n",
    "words = words.to(device)\n",
    "words_length = words_length.to(device)\n",
    "idxs = idxs.to(device)\n",
    "labels = labels.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024, 2])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = mlm((sentences, sentences_length, words, words_length, idxs))\n",
    "out.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetTrainer(object):\n",
    "    def __init__(self, train_loader, test_loader):\n",
    "        self.trainloader = train_loader\n",
    "        self.testloader = test_loader\n",
    "        \n",
    "    def assess(self, net, test=True, use_gpu=False, device=None):\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        loader = self.testloader if test else self.trainloader\n",
    "        for data in loader:\n",
    "            sentences, sentences_length, words, words_length, idxs, labels = data\n",
    "            if device is not None and (device.__str__() != \"cpu\") and use_gpu:\n",
    "                sentences = [sentence.to(device) for sentence in sentences]\n",
    "                sentences_length = [sl.to(device) for sl in sentences_length]\n",
    "                words = words.to(device)\n",
    "                words_length = words_length.to(device)\n",
    "                idxs = idxs.to(device)\n",
    "                labels = labels.to(device)\n",
    "            outputs = net((sentences, sentences_length, words, words_length, idxs))\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        dataset_name = 'test' if test else 'train'\n",
    "        print('Accuracy of the network on {} {} sentences: {:2.4f} %'.format(\n",
    "            total, dataset_name, 100 * correct / total))         \n",
    "            \n",
    "    def train(self, \n",
    "              config, \n",
    "              n_epoch=5, \n",
    "              use_gpu=False, \n",
    "              resume_training=False, \n",
    "              net_path=None):\n",
    "        net = MainLanguageModel(**config)\n",
    "        \n",
    "        if resume_training:\n",
    "            net.load_state_dict(torch.load(net_path))          \n",
    "        \n",
    "        if use_gpu:\n",
    "            device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "            if device.__str__() != \"cpu\":\n",
    "                net.to(device)\n",
    "        \n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(net.parameters(), lr=0.0002, amsgrad=True)\n",
    "\n",
    "        for epoch in range(n_epoch):\n",
    "            running_loss = 0.0\n",
    "            t = time.time()\n",
    "            net.train()\n",
    "            \n",
    "            for i, data in enumerate(self.trainloader, 0):\n",
    "                sentences, sentences_length, words, words_length, idxs,  labels = data\n",
    "                \n",
    "                if use_gpu:\n",
    "                    if(device.__str__() != \"cpu\"):\n",
    "                        sentences = [sentence.to(device) for sentence in sentences]\n",
    "                        sentences_length = [sl.to(device) for sl in sentences_length]\n",
    "                        words = words.to(device)\n",
    "                        words_length = words_length.to(device)\n",
    "                        idxs = idxs.to(device)\n",
    "                        labels = labels.to(device)\n",
    "                    \n",
    "                optimizer.zero_grad()\n",
    "                outputs = net((sentences, sentences_length, words, words_length, idxs))\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "                if i % 10 == 9:\n",
    "                    now = datetime.datetime.now()\n",
    "                    print('[%s , %d, %5d] Loss: %.4f' %\n",
    "                          (now.strftime('%Y-%m-%d %H:%M:%S'), epoch + 1, i + 1, running_loss / 100))\n",
    "                    print('[%s , %d, %5d] Elapsed time: %2.4f s' %\n",
    "                          (now.strftime('%Y-%m-%d %H:%M:%S'), epoch + 1, i + 1, time.time() - t))\n",
    "                    running_loss = 0.0\n",
    "                    t = time.time()\n",
    "            \n",
    "            net.eval()\n",
    "            with torch.no_grad():\n",
    "                if use_gpu:\n",
    "                    self.assess(net, use_gpu=True, device=device)\n",
    "                    #self.assess(net, test=False, use_gpu=True, device=device)                \n",
    "                else:\n",
    "                    self.assess(net)\n",
    "                    #self.assess(net, test=False)\n",
    "\n",
    "        return net\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'input_size': 36,\n",
    "    'embedding_size': 200,\n",
    "    'hidden_size': 100,\n",
    "    'linear_sizes': (400, 200, 100),\n",
    "    'n_classes': 2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = NetTrainer(dataloader_train, dataloader_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/home/md359230/DeepLearning/assignment3/src/model.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-06-03 16:17:28 , 1,    10] Loss: 0.0623\n",
      "[2019-06-03 16:17:28 , 1,    10] Elapsed time: 89.4656 s\n",
      "[2019-06-03 16:18:19 , 1,    20] Loss: 0.0622\n",
      "[2019-06-03 16:18:19 , 1,    20] Elapsed time: 51.6113 s\n",
      "[2019-06-03 16:19:18 , 1,    30] Loss: 0.0624\n",
      "[2019-06-03 16:19:18 , 1,    30] Elapsed time: 58.1421 s\n",
      "[2019-06-03 16:20:15 , 1,    40] Loss: 0.0622\n",
      "[2019-06-03 16:20:15 , 1,    40] Elapsed time: 57.3205 s\n",
      "[2019-06-03 16:21:14 , 1,    50] Loss: 0.0619\n",
      "[2019-06-03 16:21:14 , 1,    50] Elapsed time: 58.7944 s\n",
      "[2019-06-03 16:22:10 , 1,    60] Loss: 0.0622\n",
      "[2019-06-03 16:22:10 , 1,    60] Elapsed time: 56.2893 s\n",
      "[2019-06-03 16:23:06 , 1,    70] Loss: 0.0618\n",
      "[2019-06-03 16:23:06 , 1,    70] Elapsed time: 56.4797 s\n",
      "[2019-06-03 16:24:05 , 1,    80] Loss: 0.0615\n",
      "[2019-06-03 16:24:05 , 1,    80] Elapsed time: 58.1032 s\n",
      "[2019-06-03 16:25:02 , 1,    90] Loss: 0.0613\n",
      "[2019-06-03 16:25:02 , 1,    90] Elapsed time: 57.5769 s\n",
      "[2019-06-03 16:26:02 , 1,   100] Loss: 0.0613\n",
      "[2019-06-03 16:26:02 , 1,   100] Elapsed time: 59.5293 s\n",
      "Accuracy of the network on 26345 test sentences: 60.4745 %\n",
      "[2019-06-03 16:29:16 , 2,    10] Loss: 0.0607\n",
      "[2019-06-03 16:29:16 , 2,    10] Elapsed time: 91.6101 s\n",
      "[2019-06-03 16:30:19 , 2,    20] Loss: 0.0608\n",
      "[2019-06-03 16:30:19 , 2,    20] Elapsed time: 62.7845 s\n",
      "[2019-06-03 16:31:19 , 2,    30] Loss: 0.0602\n",
      "[2019-06-03 16:31:19 , 2,    30] Elapsed time: 60.1559 s\n",
      "[2019-06-03 16:32:19 , 2,    40] Loss: 0.0612\n",
      "[2019-06-03 16:32:19 , 2,    40] Elapsed time: 59.5529 s\n",
      "[2019-06-03 16:33:19 , 2,    50] Loss: 0.0609\n",
      "[2019-06-03 16:33:19 , 2,    50] Elapsed time: 60.2875 s\n",
      "[2019-06-03 16:34:18 , 2,    60] Loss: 0.0612\n",
      "[2019-06-03 16:34:18 , 2,    60] Elapsed time: 59.5685 s\n",
      "[2019-06-03 16:35:07 , 2,    70] Loss: 0.0598\n",
      "[2019-06-03 16:35:07 , 2,    70] Elapsed time: 48.9246 s\n",
      "[2019-06-03 16:35:57 , 2,    80] Loss: 0.0600\n",
      "[2019-06-03 16:35:57 , 2,    80] Elapsed time: 49.7425 s\n",
      "[2019-06-03 16:36:49 , 2,    90] Loss: 0.0607\n",
      "[2019-06-03 16:36:49 , 2,    90] Elapsed time: 52.0640 s\n",
      "[2019-06-03 16:37:41 , 2,   100] Loss: 0.0593\n",
      "[2019-06-03 16:37:41 , 2,   100] Elapsed time: 52.1825 s\n",
      "Accuracy of the network on 26345 test sentences: 62.3040 %\n",
      "[2019-06-03 16:40:48 , 3,    10] Loss: 0.0597\n",
      "[2019-06-03 16:40:48 , 3,    10] Elapsed time: 95.6275 s\n",
      "[2019-06-03 16:41:47 , 3,    20] Loss: 0.0600\n",
      "[2019-06-03 16:41:47 , 3,    20] Elapsed time: 58.4225 s\n",
      "[2019-06-03 16:42:44 , 3,    30] Loss: 0.0591\n",
      "[2019-06-03 16:42:44 , 3,    30] Elapsed time: 57.2486 s\n",
      "[2019-06-03 16:43:41 , 3,    40] Loss: 0.0598\n",
      "[2019-06-03 16:43:41 , 3,    40] Elapsed time: 57.2875 s\n",
      "[2019-06-03 16:44:39 , 3,    50] Loss: 0.0595\n",
      "[2019-06-03 16:44:39 , 3,    50] Elapsed time: 57.5418 s\n",
      "[2019-06-03 16:45:37 , 3,    60] Loss: 0.0593\n",
      "[2019-06-03 16:45:37 , 3,    60] Elapsed time: 58.1408 s\n",
      "[2019-06-03 16:46:35 , 3,    70] Loss: 0.0591\n",
      "[2019-06-03 16:46:35 , 3,    70] Elapsed time: 58.6017 s\n",
      "[2019-06-03 16:47:31 , 3,    80] Loss: 0.0584\n",
      "[2019-06-03 16:47:31 , 3,    80] Elapsed time: 55.8684 s\n",
      "[2019-06-03 16:48:28 , 3,    90] Loss: 0.0585\n",
      "[2019-06-03 16:48:28 , 3,    90] Elapsed time: 56.5794 s\n",
      "[2019-06-03 16:49:17 , 3,   100] Loss: 0.0588\n",
      "[2019-06-03 16:49:17 , 3,   100] Elapsed time: 48.8490 s\n",
      "Accuracy of the network on 26345 test sentences: 63.9135 %\n"
     ]
    }
   ],
   "source": [
    "model = trainer.train(config, n_epoch=3, use_gpu=True, resume_training=True, net_path=model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/home/md359230/DeepLearning/assignment3/src/model.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MainLanguageModel(\n",
       "  (wordembedder): WordEmbedder(\n",
       "    (lstm): LSTM(36, 200)\n",
       "  )\n",
       "  (lstm): LSTM(200, 100, bidirectional=True)\n",
       "  (fc): LinearStack(\n",
       "    (linear_layers): ModuleList(\n",
       "      (0): Sequential(\n",
       "        (0): Linear(in_features=400, out_features=200, bias=True)\n",
       "        (1): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): LeakyReLU(negative_slope=0.01)\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): Linear(in_features=200, out_features=100, bias=True)\n",
       "        (1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): LeakyReLU(negative_slope=0.01)\n",
       "      )\n",
       "      (2): Linear(in_features=100, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_loaded = MainLanguageModel(**config)\n",
    "model_loaded.load_state_dict(torch.load(model_path))\n",
    "model_loaded.eval()\n",
    "model_loaded.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on 26345 test sentences: 63.1505 %\n"
     ]
    }
   ],
   "source": [
    "trainer = NetTrainer(dataloader_train, dataloader_test)\n",
    "trainer.assess(model_loaded, use_gpu=True, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~63.7616 %"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network visualzation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing zone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = set()\n",
    "corpus_words = set()\n",
    "lens = list()\n",
    "with open('/home/md359230/DeepLearning/assignment3/data/task3_sample.txt', 'r') as sample_data:\n",
    "    for line in sample_data:\n",
    "        line = CorpusPreprocessor.transform_text(line)\n",
    "        lens.append(len(line))\n",
    "        chars = chars | set(line)\n",
    "        words = line.split()\n",
    "        corpus_words = corpus_words | set(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([7.0255e+04, 2.5162e+04, 3.7240e+03, 6.8500e+02, 1.3800e+02,\n",
       "        2.6000e+01, 5.0000e+00, 4.0000e+00, 0.0000e+00, 1.0000e+00]),\n",
       " array([   4.,  137.,  270.,  403.,  536.,  669.,  802.,  935., 1068.,\n",
       "        1201., 1334.]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'po wybuchu ii wojny światowej podczas kampanii wrześniowej sprawował stanowisko dowódcy pułku piechoty'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = 'Po wybuchu II wojny światowej 1939 podczas kampanii wrześniowej sprawował stanowisko dowódcy 78 pułku piechoty.'\n",
    "cp.transform_text(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        self.dictionary = OrderedDict()\n",
    "        self.corpus_path = corpus_path\n",
    "        with open(corpus_path, 'r') as corpus:\n",
    "            for line in corpus:\n",
    "                self.dictionary[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'afs 41 2 5$#sa 24 Sfw15 gw g4gfdsaf j'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "double_spaces = re.compile('\\s+')\n",
    "re.sub(double_spaces, ' ', 'afs  41 2 5$#sa 24 Sfw15 gw g4gfdsaf j')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 2])"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([torch.Size([4, 100]),\n",
       "  torch.Size([6, 100]),\n",
       "  torch.Size([5, 100]),\n",
       "  torch.Size([14, 100]),\n",
       "  torch.Size([9, 100]),\n",
       "  torch.Size([18, 100]),\n",
       "  torch.Size([6, 100]),\n",
       "  torch.Size([22, 100]),\n",
       "  torch.Size([9, 100]),\n",
       "  torch.Size([11, 100]),\n",
       "  torch.Size([6, 100]),\n",
       "  torch.Size([13, 100]),\n",
       "  torch.Size([8, 100]),\n",
       "  torch.Size([6, 100]),\n",
       "  torch.Size([25, 100]),\n",
       "  torch.Size([16, 100])],\n",
       " torch.Size([16, 100]))"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[o.size() for o in out[0]], out[1].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_words= out[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = torch.LongTensor([o.size()[0] for o in out[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_input = utils_rnn.pad_sequence(out[0])\n",
    "\n",
    "words_input.size()\n",
    "\n",
    "packed_input = utils_rnn.pack_padded_sequence(input=words_input, \n",
    "                                             lengths=lengths, \n",
    "                                             enforce_sorted=False)\n",
    "\n",
    "lstm2 = nn.LSTM(input_size=100,\n",
    "                hidden_size=50,\n",
    "                bidirectional=True)\n",
    "\n",
    "out, _ = lstm2(packed_input)\n",
    "result, _ = utils_rnn.pad_packed_sequence(out, \n",
    "                                    total_length=max(lengths).item())\n",
    "idx = (idxs).view(-1, 1)\\\n",
    "                                     .expand(len(lengths), \n",
    "                                             100)\n",
    "idx = idx.unsqueeze(0)\n",
    "result_final = result.gather(0, idx).squeeze(0)\n",
    "\n",
    "result_final.size()\n",
    "\n",
    "joined_information = torch.cat((result_final, masked_words), dim=1)\n",
    "\n",
    "fc = LinearStack((200, 100), 2)\n",
    "\n",
    "final = fc(joined_information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 2])"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0788,  0.2965],\n",
       "        [ 0.3925,  0.1174],\n",
       "        [ 0.0335, -0.1361],\n",
       "        [ 0.2581,  0.1808],\n",
       "        [ 0.1624,  0.1204],\n",
       "        [ 0.2782,  0.2596],\n",
       "        [-0.0378,  0.1297],\n",
       "        [ 0.5840,  0.2376],\n",
       "        [ 0.2791,  0.1616],\n",
       "        [ 0.5826,  0.6866],\n",
       "        [ 0.0406,  0.0681],\n",
       "        [ 0.5139, -0.0842],\n",
       "        [-0.0675,  0.0903],\n",
       "        [ 0.1305,  0.1092],\n",
       "        [ 0.4054,  0.4469],\n",
       "        [ 0.0595,  0.0318]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6807, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = nn.CrossEntropyLoss()\n",
    "loss(final, labels.squeeze(1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
