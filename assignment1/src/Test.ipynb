{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import time\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "import torch.nn.parameter as P\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "example, no = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 3, 100, 100])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 3, 100, 100])\n",
      "torch.Size([128, 8, 96, 96])\n",
      "torch.Size([128, 8, 48, 48])\n",
      "torch.Size([128, 16, 38, 38])\n",
      "torch.Size([4, 32, 23104])\n",
      "torch.Size([128, 16, 19, 19])\n"
     ]
    }
   ],
   "source": [
    "tmp = example\n",
    "print(tmp.size())\n",
    "tmp = nn.Conv2d(3, 8, 5)(tmp)\n",
    "print(tmp.size())\n",
    "tmp = nn.MaxPool2d((2,2))(tmp)\n",
    "print(tmp.size())\n",
    "tmp = nn.Conv2d(8, 16, 11)(tmp)\n",
    "print(tmp.size())\n",
    "print(tmp.view(4,-1, 16 * 38 * 38).size())\n",
    "tmp = nn.MaxPool2d(2,2)(tmp)\n",
    "print(tmp.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49191\n"
     ]
    }
   ],
   "source": [
    "! ls -R /home/md359230/DeepLearning/assignment1/data/fruits-360/Training/ | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16707\n"
     ]
    }
   ],
   "source": [
    "! ls -R /home/md359230/DeepLearning/assignment1/data/fruits-360/Test/ | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building blocks of the architecture "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyBatchNormalization2d(nn.Module):\n",
    "    \"\"\"Class containing custom implementation of batch normalization.\n",
    "    \"\"\"\n",
    "    def __init__(self, size, epsilon=1e-5):\n",
    "        \"\"\"Derived constructor.\n",
    "        \n",
    "        :param size: Number of channels in input tensor.\n",
    "        :param epsilon: Number to add to standard deviation to aviod zeros.\n",
    "        :return: MyBatchNormalization2d object.\"\"\"\n",
    "        super(MyBatchNormalization2d, self).__init__()\n",
    "        self.register_buffer('epsilon', torch.Tensor([epsilon]))\n",
    "        self.shift = P.Parameter(torch.Tensor(size))\n",
    "        self.scale = P.Parameter(torch.Tensor(size))\n",
    "        init.zeros_(self.shift)\n",
    "        init.uniform_(self.scale)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"Method implementing batch normalization logic.\n",
    "        \n",
    "        :param x: Input tensor of shape (N x C x W x H).\n",
    "        :return: Batch normalized x.\"\"\"\n",
    "        # Compute means and stds per channel\n",
    "        batch_size, channels, width, height = x.size()\n",
    "        \n",
    "        x_agg = x.permute((1, 0, 2, 3))\\\n",
    "                 .contiguous()\\\n",
    "                 .view(-1, batch_size * width * height)\n",
    "        \n",
    "        stds = torch.std(x_agg, \n",
    "                         dim=1, \n",
    "                         unbiased=False)\n",
    "        means = torch.mean(x_agg, \n",
    "                           dim=1)\n",
    "        \n",
    "        # Scale input to have 0 mean and unit variance (remeber about epsilon!)\n",
    "        tensor_mean = x.new_ones(width, height)\n",
    "        tensor_std = torch.diag(x.new_ones(height)) \n",
    "        \n",
    "        tensor_means = torch.stack([tensor_mean * means[i] for i in range(channels)])\n",
    "        tensor_stds = torch.stack([tensor_std * (1 / (stds[i] + self.epsilon)) for i in range(channels)])\n",
    "        x = torch.matmul((x - tensor_means), tensor_stds)\n",
    "        \n",
    "        # Scale input using parameters\n",
    "        tensor_shift = torch.stack([tensor_mean * self.shift[i] for i in range(channels)])\n",
    "        tensor_scale = torch.stack([tensor_std * self.scale[i] for i in range(channels)])\n",
    "        x = torch.matmul(x, tensor_scale) + tensor_shift\n",
    "        \n",
    "        # Return the result\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyBatchNormalization1d(nn.Module):\n",
    "    def __init__(self, size, epsilon=1e-5):\n",
    "        \"\"\"Derived constructor.\n",
    "        \n",
    "        :param size: Number of linear features in input tensor.\n",
    "        :param epsilon: Number to add to standard deviation to aviod zeros.\n",
    "        :return: MyBatchNormalization2d object.\"\"\"\n",
    "        super(MyBatchNormalization1d, self).__init__()\n",
    "        self.register_buffer('epsilon', torch.Tensor([epsilon]))\n",
    "        self.shift = P.Parameter(torch.Tensor(size))\n",
    "        self.scale = P.Parameter(torch.Tensor(size))\n",
    "        init.zeros_(self.shift)\n",
    "        init.uniform_(self.scale)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"Method implementing batch normalization logic.\n",
    "        \n",
    "        :param x: Input tensor of shape (N x L).\n",
    "        :return: Batch normalized x.\"\"\"\n",
    "        # Compute means and stds per batch\n",
    "        batch_size, features_size = x.size()\n",
    "        \n",
    "        stds = torch.std(x, \n",
    "                         dim=0, \n",
    "                         unbiased=False)\n",
    "        means = torch.mean(x, \n",
    "                           dim=0)\n",
    "        #print('means')\n",
    "        #print(means)\n",
    "        #print('stds')\n",
    "        #print(stds)\n",
    "\n",
    "        # Scale input to have 0 mean and unit variance (remeber about epsilon!)\n",
    "        tensor_means = torch.stack([means for _ in range(batch_size)])\n",
    "        tensor_stds = torch.diag(1 / (stds + self.epsilon))\n",
    "        \n",
    "        #print(tensor_means)\n",
    "        #print(tensor_stds)\n",
    "        x = torch.matmul((x - tensor_means), tensor_stds)\n",
    "        \n",
    "        # Scale input using parameters\n",
    "        tensor_shift = torch.stack([self.shift for _ in range(batch_size)])\n",
    "        tensor_scale = torch.diag(self.scale)\n",
    "        \n",
    "        #print(tensor_shift)\n",
    "        #print(tensor_scale)\n",
    "        x = torch.matmul(x, tensor_scale) + tensor_shift\n",
    "        \n",
    "        # Return the result\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolutional_layer(in_channels, \n",
    "                        out_channels,\n",
    "                        kernel_size,\n",
    "                        batchnorm_module='default', \n",
    "                        activation_function='relu', \n",
    "                        *args, \n",
    "                        **kwargs): \n",
    "    \"\"\"Function for flexible construction of convolutional layers for the network.\n",
    "    \n",
    "    :param in_channels: Number of input channels.\n",
    "    :param out_channels: Number of channels produced by the output.\n",
    "    :param kernel_size: Kernel size for convolutional layer.\n",
    "    :param batchnorm_module: Must be in ('default', 'custom'). Should Pytorch's or own BN \n",
    "    normalization be used?\n",
    "    :param activation_function: Currently unused.\n",
    "    :param *args: Additional arguments to pass to convolutional layer.\n",
    "    :param **kwargs: Additional keyword arguments to pass to convolutional layer.\n",
    "    :return: Layer consisting of convolution, batch normalization and max pooling.\"\"\"\n",
    "    batchnorm_modules = nn.ModuleDict([\n",
    "        ['default', nn.BatchNorm2d(out_channels)],\n",
    "        ['custom', MyBatchNormalization2d(out_channels)]\n",
    "    ])\n",
    "    \n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, kernel_size, *args, **kwargs),\n",
    "        batchnorm_modules[batchnorm_module],\n",
    "        nn.MaxPool2d((2,2))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_layer(in_features, \n",
    "                 out_feautres, \n",
    "                 batchnorm_module='default',\n",
    "                 activation_function='relu', \n",
    "                 *args, \n",
    "                 **kwargs):\n",
    "    \"\"\"Function for flexible construction of full-connected layers for the network.\n",
    "    \n",
    "    :param in_features: Size of input layer.\n",
    "    :param out_features: Size of output layer.\n",
    "    :param batchnorm_module: Must be in ('default', 'custom'). Should Pytorch's or own BN \n",
    "    normalization be used?\n",
    "    :param activation_fuction: Must be in ('relu', 'lrelu', 'sigmoid'). Which activation\n",
    "    function should be used?\n",
    "    :param *args: Additional arguments to pass to full-connected layer.\n",
    "    :param **kwargs: Additional keyword arguments to pass to full-connected layer.\n",
    "    :return: Layer consisting of full-connected module, batch normalization and activation function.\"\"\"\n",
    "    batchnorm_modules = nn.ModuleDict([\n",
    "        ['default', nn.BatchNorm1d(out_feautres)],\n",
    "        ['custom', MyBatchNormalization1d(out_feautres)]\n",
    "    ])\n",
    "    \n",
    "    activation_functions = nn.ModuleDict([\n",
    "        ['lrelu', nn.LeakyReLU()],\n",
    "        ['relu', nn.ReLU()],\n",
    "        ['sigmoid', nn.Sigmoid()]\n",
    "    ])\n",
    "\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(in_features, out_feautres, *args, **kwargs),\n",
    "        batchnorm_modules[batchnorm_module],\n",
    "        activation_functions[activation_function]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionalStack(nn.Module):\n",
    "    \"\"\"Class containing implementation of standard convolutional stack.\n",
    "    \"\"\"\n",
    "    def __init__(self, sizes, kernel_sizes, *args, **kwargs):\n",
    "        \"\"\"Derived constructor.\n",
    "        \n",
    "        :param sizes: iterable with sizes of subsequent convolutional_layers.\n",
    "        :param kernel_sizes: iterable with sizer of kernels used in subsequent\n",
    "        convolutional layers.\n",
    "        :param *args: Additional arguments to pass to convolutional layer.\n",
    "        :param **kwargs: Additional keyword arguments to pass to convolutional layer.\n",
    "        :return: ConvolutionalStack object.\"\"\"\n",
    "        super(ConvolutionalStack, self).__init__()\n",
    "        self.convolutional_layers = nn.ModuleList([convolutional_layer(in_size, out_size, ker_size, *args, **kwargs)\n",
    "                                                   for in_size, out_size, ker_size in zip(sizes, sizes[1:], kernel_sizes)])\n",
    "    def forward(self, x):\n",
    "        \"\"\"Method implemention convolutional stack forward pass.\n",
    "        :param x: Input tensor.\n",
    "        :return: Processed tensor.\"\"\"\n",
    "        for convolutional_layer in self.convolutional_layers:\n",
    "            x = convolutional_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearStack(nn.Module):\n",
    "    \"\"\"Class containing implementation of standard linear stack.\n",
    "    \"\"\"\n",
    "    def __init__(self, sizes, n_classes, *args, **kwargs):\n",
    "        \"\"\"Derived constructor.\n",
    "        \n",
    "        :param sizes: iterable with sizes of subsequent linear_layers.\n",
    "        :param *args: Additional arguments to pass to linear layer.\n",
    "        :param **kwargs: Additional keyword arguments to pass to linear layer.\n",
    "        :return: LinearStack object.\"\"\"\n",
    "        super(LinearStack, self).__init__()\n",
    "        self.linear_layers = nn.ModuleList([linear_layer(in_size, out_size, *args, **kwargs)\n",
    "                                                   for in_size, out_size in zip(sizes, sizes[1:])])\n",
    "        self.linear_layers.append(nn.Linear(sizes[-1], n_classes))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"Method implemention linear stack forward pass.\n",
    "        :param x: Input tensor.\n",
    "        :return: Processed tensor.\"\"\"\n",
    "        for linear_layer in self.linear_layers:\n",
    "            x = linear_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network and trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FruitNet(nn.Module):\n",
    "    \"\"\"Class containing architecture of FruitNet.\n",
    "    \"\"\"\n",
    "    def __init__(self, conv_sizes, kernel_sizes, linear_sizes, n_classes, *args, **kwargs):\n",
    "        \"\"\"Derived contructor.\n",
    "        \n",
    "        :param con_sizes: Number of channels in subsequent convolutional blocks.\n",
    "        :param kernel_sizes: Kernel sizes in subsequent convolutional bocks.\n",
    "        :param linear_sizes: Size of subsequent hidden layers.\n",
    "        :param n_classes: Size of output layer.\n",
    "        :param *args: Additional arguments to pass to network blocks.\n",
    "        :param **kwargs: Additional keyword arguments to pass to network blocks.\n",
    "        :return: FruitNet object\"\"\"\n",
    "        super(FruitNet, self).__init__()\n",
    "        self.conv = ConvolutionalStack(conv_sizes, kernel_sizes, *args, **kwargs)\n",
    "        self.fc = LinearStack(linear_sizes, n_classes, *args, **kwargs)\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        \"\"\"Utility method to compute size Tensor per batch instance.\n",
    "        \n",
    "        :param x: Input tensor.\n",
    "        :return: Size of Tensor per batch instance.\"\"\"\n",
    "        size = x.size()[1:] \n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Implementation of FruiNet forward pass.\n",
    "        \n",
    "        :param x: Input Tensor.\n",
    "        :return: Processed Tensor.\"\"\"\n",
    "        x = self.conv(x)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = self.fc(x)\n",
    "        x = x.squeeze()\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FruitTrainer(object):\n",
    "    \"\"\"Class designed to train FruiNet.\n",
    "    \"\"\"\n",
    "    def __init__(self, train_data, train_loader, test_data, test_loader):\n",
    "        \"\"\"Basic constructor.\n",
    "        \n",
    "        :param train_data: ImageFolder object containing data for training.\n",
    "        :param train_loader: DataLoader object containing data for training.\n",
    "        :param test_data: ImageFolder object containing data for test.\n",
    "        :param test_loader: DataLoader object containing data for test.\n",
    "        :return: FruitTrainer object.\"\"\"\n",
    "        self.trainset = train_data\n",
    "        self.trainloader = train_loader\n",
    "        \n",
    "        self.testset = test_data\n",
    "        self.testloader = test_loader\n",
    "        \n",
    "    def assess(self, net, test=True, use_gpu=False, device=None):\n",
    "        \"\"\"Method for assessment of Neural Network.\n",
    "        \n",
    "        :param net: Network to assess.\n",
    "        :param test: Boolean: If True, assessment is done on test set, otherwise on train set.\n",
    "        :param use_gpu: Is training happening of gpu?\n",
    "        :param device: Device to use in case training is done on gpu.\n",
    "        :return: Prints evaluation on standard output.\"\"\" \n",
    "        correct = 0\n",
    "        total = 0\n",
    "        loader = self.testloader if test else self.trainloader\n",
    "        for data in loader:\n",
    "            images, labels = data\n",
    "            if device is not None and (device.__str__() != \"cpu\") and use_gpu:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        dataset_name = 'test' if test else 'train'\n",
    "        print('Accuracy of the network on {} {} images: {:2.4f} %'.format(\n",
    "            total, dataset_name, 100 * correct / total))         \n",
    "            \n",
    "    def train(self, config, n_epoch=5, use_gpu=False):\n",
    "        net = FruitNet(**config)\n",
    "        \n",
    "        if use_gpu:\n",
    "            device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "            if device.__str__() != \"cpu\":\n",
    "                net.to(device)\n",
    "        \n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(net.parameters(), lr=0.00015, amsgrad=True)\n",
    "\n",
    "        for epoch in range(n_epoch):\n",
    "            running_loss = 0.0\n",
    "            t = time.time()\n",
    "            for i, data in enumerate(self.trainloader, 0):\n",
    "                inputs, labels = data\n",
    "                \n",
    "                if use_gpu:\n",
    "                    if(device.__str__() != \"cpu\"):\n",
    "                        inputs, labels = inputs.to(device), labels.to(device)\n",
    "                    \n",
    "                optimizer.zero_grad()\n",
    "                outputs = net(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "                if i % 50 == 49:\n",
    "                    now = datetime.datetime.now()\n",
    "                    print('[%s , %d, %5d] Loss: %.4f' %\n",
    "                          (now.strftime('%Y-%m-%d %H:%M:%S'), epoch + 1, i + 1, running_loss / 100))\n",
    "                    print('[%s , %d, %5d] Elapsed time: %2.4f s' %\n",
    "                          (now.strftime('%Y-%m-%d %H:%M:%S'), epoch + 1, i + 1, time.time() - t))\n",
    "                    running_loss = 0.0\n",
    "                    t = time.time()\n",
    "            \n",
    "            if use_gpu:\n",
    "                self.assess(net, use_gpu=True, device=device)\n",
    "                self.assess(net, test=False, use_gpu=True, device=device)                \n",
    "            else:\n",
    "                self.assess(net)\n",
    "                self.assess(net, test=False)\n",
    "        \n",
    "        return net\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Neural Network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = datasets.ImageFolder(os.path.normpath('/home/md359230/DeepLearning/assignment1/data/fruits-360/Training'),\n",
    "                                  transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_data,\n",
    "                                          batch_size=256,\n",
    "                                          shuffle=True,\n",
    "                                          num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = datasets.ImageFolder(os.path.normpath('/home/md359230/DeepLearning/assignment1/data/fruits-360/Test'),\n",
    "                                  transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(test_data,\n",
    "                                          batch_size=256,\n",
    "                                          shuffle=True,\n",
    "                                          num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'conv_sizes': (3, 8, 16, 32, 64), \n",
    "    'kernel_sizes': (3, 3, 5, 5), \n",
    "    'linear_sizes': (256, 200, 150), \n",
    "    'n_classes': 95,\n",
    "    'batchnorm_module': 'custom',\n",
    "    'activation_function': 'relu'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-04-19 13:56:36 , 1,    50] Loss: 2.1089\n",
      "[2019-04-19 13:56:36 , 1,    50] Elapsed time: 9.0704 s\n",
      "[2019-04-19 13:56:44 , 1,   100] Loss: 1.8926\n",
      "[2019-04-19 13:56:44 , 1,   100] Elapsed time: 8.0945 s\n",
      "[2019-04-19 13:56:52 , 1,   150] Loss: 1.7265\n",
      "[2019-04-19 13:56:52 , 1,   150] Elapsed time: 7.8193 s\n",
      "Accuracy of the network on 16445 test images: 72.2104 %\n",
      "Accuracy of the network on 48905 train images: 74.8799 %\n",
      "[2019-04-19 13:57:30 , 2,    50] Loss: 1.4543\n",
      "[2019-04-19 13:57:30 , 2,    50] Elapsed time: 8.7154 s\n",
      "[2019-04-19 13:57:38 , 2,   100] Loss: 1.3096\n",
      "[2019-04-19 13:57:38 , 2,   100] Elapsed time: 8.2009 s\n",
      "[2019-04-19 13:57:46 , 2,   150] Loss: 1.1763\n",
      "[2019-04-19 13:57:46 , 2,   150] Elapsed time: 7.8041 s\n",
      "Accuracy of the network on 16445 test images: 84.5303 %\n",
      "Accuracy of the network on 48905 train images: 88.0973 %\n",
      "[2019-04-19 13:58:26 , 3,    50] Loss: 0.9507\n",
      "[2019-04-19 13:58:26 , 3,    50] Elapsed time: 12.1332 s\n",
      "[2019-04-19 13:58:38 , 3,   100] Loss: 0.8287\n",
      "[2019-04-19 13:58:38 , 3,   100] Elapsed time: 11.3393 s\n",
      "[2019-04-19 13:58:49 , 3,   150] Loss: 0.7205\n",
      "[2019-04-19 13:58:49 , 3,   150] Elapsed time: 11.5065 s\n",
      "Accuracy of the network on 16445 test images: 90.4105 %\n",
      "Accuracy of the network on 48905 train images: 94.9760 %\n",
      "[2019-04-19 13:59:29 , 4,    50] Loss: 0.5591\n",
      "[2019-04-19 13:59:29 , 4,    50] Elapsed time: 8.5069 s\n",
      "[2019-04-19 13:59:37 , 4,   100] Loss: 0.4824\n",
      "[2019-04-19 13:59:37 , 4,   100] Elapsed time: 7.8012 s\n",
      "[2019-04-19 13:59:44 , 4,   150] Loss: 0.4167\n",
      "[2019-04-19 13:59:44 , 4,   150] Elapsed time: 7.8292 s\n",
      "Accuracy of the network on 16445 test images: 93.1225 %\n",
      "Accuracy of the network on 48905 train images: 97.3663 %\n",
      "[2019-04-19 14:00:22 , 5,    50] Loss: 0.3219\n",
      "[2019-04-19 14:00:22 , 5,    50] Elapsed time: 8.7219 s\n",
      "[2019-04-19 14:00:29 , 5,   100] Loss: 0.2707\n",
      "[2019-04-19 14:00:29 , 5,   100] Elapsed time: 7.4414 s\n",
      "[2019-04-19 14:00:38 , 5,   150] Loss: 0.2387\n",
      "[2019-04-19 14:00:38 , 5,   150] Elapsed time: 8.8170 s\n",
      "Accuracy of the network on 16445 test images: 94.9833 %\n",
      "Accuracy of the network on 48905 train images: 99.0185 %\n",
      "[2019-04-19 14:01:22 , 6,    50] Loss: 0.1999\n",
      "[2019-04-19 14:01:22 , 6,    50] Elapsed time: 15.7263 s\n",
      "[2019-04-19 14:01:34 , 6,   100] Loss: 0.1625\n",
      "[2019-04-19 14:01:34 , 6,   100] Elapsed time: 12.0658 s\n",
      "[2019-04-19 14:01:46 , 6,   150] Loss: 0.1406\n",
      "[2019-04-19 14:01:46 , 6,   150] Elapsed time: 12.5066 s\n",
      "Accuracy of the network on 16445 test images: 95.5366 %\n",
      "Accuracy of the network on 48905 train images: 99.4193 %\n",
      "[2019-04-19 14:02:26 , 7,    50] Loss: 0.1286\n",
      "[2019-04-19 14:02:26 , 7,    50] Elapsed time: 9.1133 s\n",
      "[2019-04-19 14:02:34 , 7,   100] Loss: 0.1029\n",
      "[2019-04-19 14:02:34 , 7,   100] Elapsed time: 8.6635 s\n",
      "[2019-04-19 14:02:43 , 7,   150] Loss: 0.0931\n",
      "[2019-04-19 14:02:43 , 7,   150] Elapsed time: 8.5572 s\n",
      "Accuracy of the network on 16445 test images: 96.5704 %\n",
      "Accuracy of the network on 48905 train images: 99.7444 %\n",
      "[2019-04-19 14:03:21 , 8,    50] Loss: 0.0853\n",
      "[2019-04-19 14:03:21 , 8,    50] Elapsed time: 8.4693 s\n",
      "[2019-04-19 14:03:29 , 8,   100] Loss: 0.0706\n",
      "[2019-04-19 14:03:29 , 8,   100] Elapsed time: 8.2562 s\n",
      "[2019-04-19 14:03:37 , 8,   150] Loss: 0.0633\n",
      "[2019-04-19 14:03:37 , 8,   150] Elapsed time: 8.1444 s\n",
      "Accuracy of the network on 16445 test images: 96.7589 %\n",
      "Accuracy of the network on 48905 train images: 99.8855 %\n",
      "[2019-04-19 14:04:14 , 9,    50] Loss: 0.0698\n",
      "[2019-04-19 14:04:14 , 9,    50] Elapsed time: 8.7542 s\n",
      "[2019-04-19 14:04:22 , 9,   100] Loss: 0.0512\n",
      "[2019-04-19 14:04:22 , 9,   100] Elapsed time: 7.9745 s\n",
      "[2019-04-19 14:04:29 , 9,   150] Loss: 0.0460\n",
      "[2019-04-19 14:04:29 , 9,   150] Elapsed time: 7.8894 s\n",
      "Accuracy of the network on 16445 test images: 96.7102 %\n",
      "Accuracy of the network on 48905 train images: 99.9305 %\n",
      "[2019-04-19 14:05:06 , 10,    50] Loss: 0.0581\n",
      "[2019-04-19 14:05:06 , 10,    50] Elapsed time: 8.3872 s\n",
      "[2019-04-19 14:05:14 , 10,   100] Loss: 0.0383\n",
      "[2019-04-19 14:05:14 , 10,   100] Elapsed time: 7.8248 s\n",
      "[2019-04-19 14:05:22 , 10,   150] Loss: 0.0358\n",
      "[2019-04-19 14:05:22 , 10,   150] Elapsed time: 8.0012 s\n",
      "Accuracy of the network on 16445 test images: 96.4670 %\n",
      "Accuracy of the network on 48905 train images: 99.9427 %\n",
      "[2019-04-19 14:05:59 , 11,    50] Loss: 0.0552\n",
      "[2019-04-19 14:05:59 , 11,    50] Elapsed time: 8.2442 s\n",
      "[2019-04-19 14:06:06 , 11,   100] Loss: 0.0316\n",
      "[2019-04-19 14:06:06 , 11,   100] Elapsed time: 7.7188 s\n",
      "[2019-04-19 14:06:13 , 11,   150] Loss: 0.0280\n",
      "[2019-04-19 14:06:13 , 11,   150] Elapsed time: 7.0544 s\n",
      "Accuracy of the network on 16445 test images: 97.0812 %\n",
      "Accuracy of the network on 48905 train images: 99.9755 %\n",
      "[2019-04-19 14:06:50 , 12,    50] Loss: 0.0336\n",
      "[2019-04-19 14:06:50 , 12,    50] Elapsed time: 8.3573 s\n",
      "[2019-04-19 14:06:58 , 12,   100] Loss: 0.0248\n",
      "[2019-04-19 14:06:58 , 12,   100] Elapsed time: 7.8689 s\n",
      "[2019-04-19 14:07:06 , 12,   150] Loss: 0.0228\n",
      "[2019-04-19 14:07:06 , 12,   150] Elapsed time: 8.0164 s\n",
      "Accuracy of the network on 16445 test images: 97.3305 %\n",
      "Accuracy of the network on 48905 train images: 99.9898 %\n"
     ]
    }
   ],
   "source": [
    "trainer = FruitTrainer(train_data, train_loader, test_data, test_loader)\n",
    "model = trainer.train(config, n_epoch=12, use_gpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.path.join(os.getcwd(), 'model.pkl')\n",
    "pickle.dump(model, open(model_path, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load saved Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_loaded = pickle.load(open(model_path, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on 16445 test images: 97.3670 %\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "model_loaded.to(device)\n",
    "trainer.assess(model_loaded, test=True, use_gpu=True, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OcclusionVisualizer(object):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 3, 100, 100])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def fruitImShow(img):\n",
    "    npimg = img.numpy()\n",
    "    img = plt.imshow(npimg)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "for i, images in enumerate(train_loader):\n",
    "    if i >= 0:\n",
    "        print(images[0].squeeze().size())\n",
    "        fruitImShow(np.transpose(images[0][0,:,:,:], [1, 2, 0]))\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing zone: Keep away! You enter at your own risk. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[[[ 0.4801, -1.0264],\n",
    "          [-0.6684, -0.0054]],\n",
    "\n",
    "         [[-0.3160, -0.1503],\n",
    "          [-0.8476,  0.9424]],\n",
    "\n",
    "         [[-0.5158,  0.4040],\n",
    "          [ 1.5077, -0.5627]]],\n",
    "\n",
    "\n",
    "        [[[-1.1366,  0.7921],\n",
    "          [ 1.4411, -1.6859]],\n",
    "\n",
    "         [[-0.6283,  0.7755],\n",
    "          [ 0.7855,  0.6781]],\n",
    "\n",
    "         [[ 0.2961, -0.6014],\n",
    "          [ 0.8488, -2.1111]]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.6993, -0.7923],\n",
       "          [-0.4379,  0.2186]],\n",
       "\n",
       "         [[-0.7010, -0.4543],\n",
       "          [-1.4924,  1.1723]],\n",
       "\n",
       "         [[-0.4095,  0.4788],\n",
       "          [ 1.5446, -0.4547]]],\n",
       "\n",
       "\n",
       "        [[[-0.9015,  1.0082],\n",
       "          [ 1.6508, -1.4453]],\n",
       "\n",
       "         [[-1.1659,  0.9238],\n",
       "          [ 0.9387,  0.7788]],\n",
       "\n",
       "         [[ 0.3746, -0.4921],\n",
       "          [ 0.9083, -1.9500]]]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mbn = MyBatchNormalization2d(2)\n",
    "mbn.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input\n",
      "tensor([[ 0.3534,  1.3480,  1.0362],\n",
      "        [-1.6341, -0.5642, -0.9382],\n",
      "        [-0.4853, -0.3669, -0.1098],\n",
      "        [-1.4520, -0.1399,  0.0106],\n",
      "        [ 1.3992, -0.1157, -0.3049],\n",
      "        [ 0.0403, -0.1023,  0.0091],\n",
      "        [ 0.7643,  0.0948,  1.3555],\n",
      "        [ 1.7898,  0.9649, -1.7940]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0466,  0.3784,  0.5910],\n",
       "        [-0.3145, -0.2205, -0.4434],\n",
       "        [-0.1058, -0.1587, -0.0094],\n",
       "        [-0.2814, -0.0876,  0.0537],\n",
       "        [ 0.2366, -0.0800, -0.1116],\n",
       "        [-0.0103, -0.0758,  0.0529],\n",
       "        [ 0.1212, -0.0141,  0.7583],\n",
       "        [ 0.3075,  0.2584, -0.8917]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.Tensor([[ 0.3534,  1.3480,  1.0362],\n",
    "        [-1.6341, -0.5642, -0.9382],\n",
    "        [-0.4853, -0.3669, -0.1098],\n",
    "        [-1.4520, -0.1399,  0.0106],\n",
    "        [ 1.3992, -0.1157, -0.3049],\n",
    "        [ 0.0403, -0.1023,  0.0091],\n",
    "        [ 0.7643,  0.0948,  1.3555],\n",
    "        [ 1.7898,  0.9649, -1.7940]])\n",
    "print('input')\n",
    "print(x)\n",
    "\n",
    "mbn1d = MyBatchNormalization1d(3)\n",
    "mbn1d.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:0'"
      ]
     },
     "execution_count": 447,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\").__str__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1.]])\n",
      "tensor([[ 0.0000, -0.7406, -1.3640,  0.5907],\n",
      "        [ 0.3773,  0.0000, -1.7041, -0.8587],\n",
      "        [-0.7173,  0.0738,  0.0000,  0.3572],\n",
      "        [-1.3948, -1.5232, -0.4627,  0.0000]])\n",
      "tensor([[-0.6915, -0.7406, -1.3640,  0.5907],\n",
      "        [ 0.3773, -0.7853, -1.7041, -0.8587],\n",
      "        [-0.7173,  0.0738, -2.1117,  0.3572],\n",
      "        [-1.3948, -1.5232, -0.4627, -2.2673]])\n",
      "tensor([-0.6915, -0.7853, -2.1117, -2.2673])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(4,4)\n",
    "mask = torch.diag(torch.ones(4))\n",
    "out = mask * torch.diag(torch.zeros(4)) + (1. - mask) * x\n",
    "print(mask)\n",
    "print(out)\n",
    "print(x)\n",
    "print(torch.diag(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 2, -1), (2, 3, -2), (3, 4, -3)]"
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = (1,2,3,4)\n",
    "b = (-1,-2,-3)\n",
    "list(zip(a, a[1:], b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2, 3)"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[:-1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
